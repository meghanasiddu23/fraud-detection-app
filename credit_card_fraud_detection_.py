# -*- coding: utf-8 -*-
"""Credit card fraud detection  .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VpXzOdp8yYMSNtelWgvze7dKBFY5DHZI

**Implementing ML Algorithum**
"""

import pandas as pd
df = pd.read_csv("creditcard.csv")
df

df.shape

df.isnull().sum()

amo =(df['Amount'].mean)
mer =(df['MerchantID'].mean)
tra =(df['TransactionType'].mean)
loc =(df['Location'].mean)
fra =(df['IsFraud'].mean)
td =(df['TransactionDate'].mean)

df['Amount']=df['Amount'].fillna(amo)
df['MerchantID']=df['MerchantID'].fillna(mer)
df['TransactionType']=df['TransactionType'].fillna(tra)
df['Location']=df['Location'].fillna(loc)
df['IsFraud']=df['IsFraud'].fillna(fra)
df['TransactionDate']=df['TransactionDate'].fillna(td)

df.isnull().sum()

df = df.drop_duplicates()

df.duplicated().sum()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Load dataset (Replace with actual file)
df = pd.read_csv("creditcard.csv")

# Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# Reduce dataset size for faster training
df_sample = df.sample(n=500, random_state=42)  # Adjust sample size if needed
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df_sample['IsFraud']

# Split data (60% train, 40% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)

# Check class balance before applying SMOTE
print(f" Fraud cases before SMOTE: {sum(y_train == 1)}")
print(f" Non-fraud cases before SMOTE: {sum(y_train == 0)}")

# Apply SMOTE to balance classes
smote = SMOTE(sampling_strategy=0.3, random_state=42, k_neighbors=min(2, sum(y_train == 1) - 1))
X_train, y_train = smote.fit_resample(X_train, y_train)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train SVM model
svm_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)
svm_model.fit(X_train, y_train)

# Predict on test data
y_pred = svm_model.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print results
print(f" Model Accuracy: {accuracy * 100:.2f}%")
print("\n Classification Report:\n", report)
print("\nðŸ› ï¸ Confusion Matrix:\n", conf_matrix)

# ðŸ“Š Bar Chart: Fraud vs. Non-Fraud Predictions
fraud_counts = pd.Series(y_pred).value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=fraud_counts.index, y=fraud_counts.values, palette=["blue", "red"])
plt.xticks([0, 1], ["Not Fraud", "Fraud"])
plt.xlabel("Prediction")
plt.ylabel("Count")
plt.title("Fraud Prediction Distribution (SVM)")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Load dataset (Replace with actual file)
df = pd.read_csv("creditcard.csv")

# Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# Reduce dataset size for faster training
df_sample = df.sample(n=50000, random_state=42)  # Adjust sample size if needed
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df_sample['IsFraud']

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE to balance classes
smote = SMOTE(sampling_strategy=0.3, random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ðŸš€ Train Random Forest Model
rf_model = RandomForestClassifier(
    n_estimators=100,  # Number of trees
    max_depth=10,  # Limit tree depth
    random_state=42
)

rf_model.fit(X_train, y_train)

# Predict on test data
y_pred = rf_model.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print results
print(f"Model Accuracy: {accuracy * 100:.2f}%")
print("\n Classification Report:\n", report)

# ðŸ“Œ Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# ðŸ“Š Plot Bar Graph for Predicted Fraud vs. Not Fraud
fraud_counts = pd.Series(y_pred).value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=['Not Fraud', 'Fraud'], y=fraud_counts.values, palette=['blue', 'red'])
plt.xlabel("Prediction")
plt.ylabel("Count")
plt.title("Fraud vs. Not Fraud Predictions")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE


# Load dataset (Replace with actual file)
df = pd.read_csv("creditcard.csv")

# Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# Reduce dataset size for faster training
#df_sample = df.sample(n=500, random_state=42)  # Adjust sample size if needed
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df_sample['IsFraud']

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

smote = SMOTE(sampling_strategy=0.3, k_neighbors=2, random_state=42)  # Reduce k_neighbors
X_train, y_train = smote.fit_resample(X_train, y_train)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#  Train Decision Tree Model
dt_model = DecisionTreeClassifier(
    max_depth=10,  # Limit tree depth for better generalization
    random_state=42
)

dt_model.fit(X_train, y_train)

# Predict on test data
y_pred = dt_model.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print results
print(f" Model Accuracy: {accuracy * 100:.2f}%")
print("\n Classification Report:\n", report)


# ðŸ“Œ Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Decision Tree")
plt.show()


# ðŸ“Š Plot Bar Graph
fraud_counts = df['IsFraud'].value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=fraud_counts.index, y=fraud_counts.values, palette="viridis")
plt.xlabel("Fraud Status")
plt.ylabel("Count")
plt.title("Fraud vs Non-Fraud Transactions")
plt.xticks(ticks=[0, 1], labels=['Not Fraud', 'Fraud'])
plt.show()


df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], unit='s', errors='coerce')  # Convert back to datetime
fraud_over_time = df.groupby(df['TransactionDate'].dt.date)['IsFraud'].sum()


# Plot Line Graph
plt.figure(figsize=(10, 5))
sns.lineplot(x=fraud_over_time.index, y=fraud_over_time.values, marker='o', color='red')
plt.xlabel("Transaction Date")
plt.ylabel("Number of Fraudulent Transactions")
plt.title("Fraud Transactions Over Time")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# ðŸ”¹ Load dataset
df = pd.read_csv("creditcard.csv")  # Change to your actual dataset

# ðŸ”¹ Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# ðŸ”¹ Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# ðŸ”¹ Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# ðŸ”¹ Reduce dataset size for faster execution (Optional)
#df_sample = df.sample(n=50000, random_state=42)
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# ðŸ”¹ Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ðŸ”¹ Apply SMOTE to balance classes
smote = SMOTE(sampling_strategy=0.3, random_state=42, k_neighbors=2)
X_train, y_train = smote.fit_resample(X_train, y_train)


# ðŸ”¹ Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# ðŸ”¹ Train KNN classifier
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
knn.fit(X_train, y_train)


# ðŸ”¹ Predict on test data
y_pred = knn.predict(X_test)


# ðŸ”¹ Evaluate model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)


# ðŸ”¹ Print results
print(f" Model Accuracy: {accuracy * 100:.2f}%")
print("\n Classification Report:\n", report)


# ðŸ”¹ Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Fraud", "Fraud"], yticklabels=["Not Fraud", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for KNN")
plt.show()

# Plot the results
plt.figure(figsize=(8, 5))
plt.plot(k_values, accuracy_scores, marker='o', linestyle='-', color='b', label="Accuracy")
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Accuracy")
plt.title("KNN Accuracy vs. Number of Neighbors")
plt.xticks(k_values)
plt.grid(True)
plt.legend()
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# ðŸ”¹ Load dataset
df = pd.read_csv("creditcard.csv")  # Change this to your actual dataset

# ðŸ”¹ Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# ðŸ”¹ Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# ðŸ”¹ Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# ðŸ”¹ Reduce dataset size for faster execution (Optional)
#df_sample = df.sample(n=50000, random_state=42)
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df_sample['IsFraud']

# ðŸ”¹ Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)

# ðŸ”¹ Check class distribution before applying SMOTE
print("Before SMOTE:", y_train.value_counts())

# ðŸ”¹ Apply SMOTE to balance classes (Fix: Use k_neighbors=2)
smote = SMOTE(sampling_strategy=0.3, random_state=42, k_neighbors=2)
X_train, y_train = smote.fit_resample(X_train, y_train)

# ðŸ”¹ Check class distribution after SMOTE
print("After SMOTE:", y_train.value_counts())

# ðŸ”¹ Scale features (NaÃ¯ve Bayes does not require scaling, but it helps with numerical stability)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ðŸ”¹ Train NaÃ¯ve Bayes classifier
nb = GaussianNB()
nb.fit(X_train, y_train)

# ðŸ”¹ Predict on test data
y_pred = nb.predict(X_test)

# ðŸ”¹ Evaluate model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# ðŸ”¹ Print results
print(f" Model Accuracy: {accuracy * 100:.2f}%")
print("\n Classification Report:\n", report)

# ðŸ”¹ Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Fraud", "Fraud"], yticklabels=["Not Fraud", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for NaÃ¯ve Bayes")
plt.show()


# ðŸ”¹ Bar Chart: Count of Fraud vs. Non-Fraud Predictions
fraud_counts = pd.Series(y_pred).value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=fraud_counts.index, y=fraud_counts.values, palette=["blue", "red"])
plt.xticks([0, 1], ["Not Fraud", "Fraud"])
plt.xlabel("Prediction")
plt.ylabel("Count")
plt.title("Fraud Prediction Distribution (NaÃ¯ve Bayes)")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from imblearn.over_sampling import SMOTE

# Load dataset (Replace with actual file)
df = pd.read_csv("creditcard.csv")

# Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# Reduce dataset size for faster training (optional)
df_sample = df.sample(n=500, random_state=42)  # Adjust sample size if needed
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df_sample['IsFraud']

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)

# Apply SMOTE for handling class imbalance
smote = SMOTE(sampling_strategy=0.3, k_neighbors=2, random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Logistic Regression Model
log_model = LogisticRegression(random_state=42)
log_model.fit(X_train, y_train)

# Predict on test data
y_pred = log_model.predict(X_test)
y_pred_proba = log_model.predict_proba(X_test)[:, 1]  # Get probabilities for ROC curve

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print results
print(f"Model Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", report)

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()


# ðŸ”¹ Bar Chart: Fraud vs. Non-Fraud Predictions
fraud_counts = pd.Series(y_pred).value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=fraud_counts.index, y=fraud_counts.values, palette=["blue", "red"])
plt.xticks([0, 1], ["Not Fraud", "Fraud"])
plt.xlabel("Prediction")
plt.ylabel("Count")
plt.title("Fraud Prediction Distribution (Logistic Regression)")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from imblearn.over_sampling import SMOTE
import xgboost as xgb

# Load dataset (Replace with actual file)
df = pd.read_csv("creditcard.csv")

# Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}
for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# Convert TransactionDate to numerical format
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['TransactionDate'] = df['TransactionDate'].apply(lambda x: x.timestamp() if pd.notnull(x) else 0)

# Select features and target variable
X = df[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df['IsFraud']

# Reduce dataset size for faster training (optional)
df_sample = df.sample(n=500, random_state=42)  # Adjust sample size if needed
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location', 'TransactionDate']]
y = df_sample['IsFraud']

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE for handling class imbalance
smote = SMOTE(sampling_strategy=0.3, k_neighbors=2, random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train XGBoost Model
xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# Predict on test data
y_pred = xgb_model.predict(X_test)
y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Get probabilities for ROC curve

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print results
print(f"Model Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", report)

# ðŸ“Œ Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - XGBoost")
plt.show()


#  Bar Chart: Fraud vs. Non-Fraud Predictions
fraud_counts = pd.Series(y_pred).value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=fraud_counts.index, y=fraud_counts.values, palette=["blue", "red"])
plt.xticks([0, 1], ["Not Fraud", "Fraud"])
plt.xlabel("Prediction")
plt.ylabel("Count")
plt.title("Fraud Prediction Distribution (XGBoost)")
plt.show()

# Line Graph: Fraud Probability Predictions
plt.figure(figsize=(10, 4))
plt.plot(range(len(y_pred_proba)), y_pred_proba, marker="o", linestyle="-", color="red", label="Fraud Probability")
plt.axhline(y=0.5, color="blue", linestyle="--", label="Threshold (0.5)")
plt.xlabel("Transaction Index")
plt.ylabel("Fraud Probability")
plt.title("Fraud Probability Predictions (XGBoost)")
plt.legend()
plt.show()

"""**Deep Learning**"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras.layers import Dense

# Load dataset
df = pd.read_csv("creditcard.csv")

# Display dataset info
print(df.head())
print(df.info())

# Drop irrelevant columns (TransactionID, TransactionDate)
df = df.drop(columns=["TransactionID", "TransactionDate"])

# Encode categorical variables (TransactionType, MerchantID, Location)
label_encoders = {}
categorical_features = ["TransactionType", "MerchantID", "Location"]
for col in categorical_features:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

# Split features (X) and target (y)
X = df.drop(columns=["IsFraud"]).values  # Features
y = df["IsFraud"].values  # Target variable (1 for fraud, 0 for non-fraud)

# Normalize numerical features (Amount)
scaler = StandardScaler()
X[:, 0] = scaler.fit_transform(X[:, 0].reshape(-1, 1)).flatten()  # Scaling Amount

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define neural network model
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation="relu"))  # Input layer
model.add(Dense(32, activation="relu"))
model.add(Dense(16, activation="relu"))
model.add(Dense(1, activation="sigmoid"))  # Output layer for binary classification

# Compile model
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train model
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Evaluate model
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)

# Save the trained model
model.save("dnn_model.h5")

# Make a fraud prediction for a sample transaction
sample_transaction = np.array([[500, 2, 5, 3]])  # Example values for Amount, MerchantID, TransactionType, Location
sample_transaction[:, 0] = scaler.transform(sample_transaction[:, 0].reshape(-1, 1)).flatten()  # Scale Amount
fraud_prediction = model.predict(sample_transaction)
print("Fraud Probability:", fraud_prediction[0][0])

from google.colab import files
files.download('dnn_model.h5')

# app.py or wherever you load the DNN
from tensorflow.keras.models import load_model

model = load_model('dnn_model.h5')  # Ensure the path is correct

print("Loading DNN model...")

model = load_model('dnn_model.h5')

print("DNN model loaded successfully.")

pip install pennylane pennylane-qiskit qiskit

!pip install pennylane pandas matplotlib scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pennylane as qml
from pennylane import numpy as pnp
from sklearn.base import BaseEstimator, ClassifierMixin

# Load dataset
df = pd.read_csv("creditcard.csv")  # Use your actual dataset here

# Encode categorical features (TransactionType, MerchantID, Location)
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
label_encoders = {col: LabelEncoder() for col in categorical_cols}

for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col].astype(str))

# Sample 100 rows for quantum training
df_sample = df.sample(n=100, random_state=42)  # Use a smaller sample size
X_qnn = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location']]
y_qnn = df_sample['IsFraud']

# Normalize features
scaler = StandardScaler()
X_qnn = scaler.fit_transform(X_qnn)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_qnn, y_qnn, test_size=0.4, random_state=42,  stratify=y_qnn)

import pennylane as qml
from pennylane import numpy as pnp

# Define the number of qubits
n_qubits = 4  # Number of qubits can be adjusted

# Set up the quantum device
dev = qml.device("default.qubit", wires=n_qubits)

# Define the feature map (quantum circuit to map data to qubits)
def feature_map(x):
    for i in range(n_qubits):
        qml.Hadamard(wires=i)  # Apply Hadamard gate to all qubits
        qml.RY(np.pi * x[i], wires=i)  # Apply rotation based on the input data

# Define the variational circuit (for classification)
def variational_circuit(weights):
    for i in range(n_qubits):
        qml.Rot(weights[i, 0], weights[i, 1], weights[i, 2], wires=i)  # Apply rotations to each qubit
    for i in range(n_qubits - 1):
        qml.CNOT(wires=[i, i + 1])  # Apply CNOT gates for entangling qubits

# Define the quantum neural network (QNN) circuit
@qml.qnode(dev, interface="autograd")
def qnn_circuit(inputs, weights):
    feature_map(inputs)  # Map data to quantum states
    variational_circuit(weights)  # Apply variational circuit
    return qml.expval(qml.PauliZ(0))  # Measure the state of the first qubit


# Define the QuantumClassifier class

class QuantumClassifier:
    def __init__(self, steps=50, lr=0.1):
        self.steps = steps
        self.lr = lr

    def fit(self, X, y):
        X = pnp.array(X)
        y = pnp.array(y)
        shape = (n_qubits, 3)  # Number of qubits and rotations
        weights = pnp.random.randn(*shape, requires_grad=True)  # Initialize weights
        opt = qml.GradientDescentOptimizer(stepsize=self.lr)  # Define optimizer

        # Training loop
        for step in range(self.steps):
            weights, loss = opt.step_and_cost(lambda w: self._cost(w, X, y), weights)
            if step % 10 == 0:
                print(f"Step {step} - Loss: {float(loss):.4f}")  # Print loss every 10 steps
        self.weights_ = weights  # Store trained weights
        return self

    def _cost(self, weights, X, y):
        predictions = [qnn_circuit(x, weights) for x in X]  # Get predictions
        return np.mean((predictions - y) ** 2)  # Mean squared error

    def predict(self, X):
        predictions = [qnn_circuit(x, self.weights_) for x in X]  # Make predictions
        return np.array([1 if p < 0 else 0 for p in predictions])  # Map to binary predictions

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

import pennylane as qml
import pennylane.numpy as pnp
from pennylane.optimize import AdamOptimizer

# Load and preprocess data
df = pd.read_csv("creditcard.csv")


# Encode categorical features
categorical_cols = ['MerchantID', 'TransactionType', 'Location']
for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))

# Sample a small dataset for demonstration
df_sample = df.sample(n=500, random_state=1)  # You can increase this later
X = df_sample[['Amount', 'MerchantID', 'TransactionType', 'Location']]
y = df_sample['IsFraud'].values

# Feature expansion and normalization
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X)
X_scaled = StandardScaler().fit_transform(X_poly)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=42)

# Binary label encoding for quantum regression
y_train_bin = 1 - 2 * y_train  # 0 -> 1, 1 -> -1
y_test_bin = 1 - 2 * y_test

# Quantum circuit setup
n_qubits = 4
num_layers = 2
dev = qml.device("default.qubit", wires=n_qubits)

def quantum_circuit(inputs, weights):
    qml.templates.AngleEmbedding(inputs[:n_qubits], wires=range(n_qubits))
    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))
    return qml.expval(qml.PauliZ(0))

qnode = qml.QNode(quantum_circuit, dev, interface="autograd")

def predict(X, weights):
    return pnp.array([qnode(x, weights) for x in X])

def square_loss(y_true, y_pred):
    return pnp.mean((y_true - y_pred)**2)

# Initialize trainable weights
weights = pnp.array(np.random.randn(num_layers, n_qubits, 3), requires_grad=True)

# Optimizer
opt = AdamOptimizer(stepsize=0.1)

# Training loop
epochs = 100
for epoch in range(epochs):
    weights, loss = opt.step_and_cost(lambda w: square_loss(y_train_bin, predict(X_train[:, :n_qubits], w)), weights)
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: loss = {loss:.4f}")

# Predict and evaluate
preds = predict(X_test[:, :n_qubits], weights)
pred_labels = (preds < 0).astype(int)  # Convert back from [-1,1] to binary 0/1

accuracy = accuracy_score(y_test, pred_labels)
print(f"\nâœ… Test Accuracy: {accuracy * 100:.2f}%")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, pred_labels)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Quantum Classifier")
plt.show()

